# Prometheus Alerting Rules for SingBox-Rust
#
# Recommended alerting rules for production monitoring of singbox-rust.
# These rules detect common issues and performance degradation.
#
# Usage:
#   1. Add to prometheus.yml:
#      rule_files:
#        - /path/to/grafana/alerts/rules.yml
#   2. Reload Prometheus: curl -X POST http://localhost:9090/-/reload
#   3. View alerts: http://localhost:9090/alerts
#
# Alerting Pipeline:
#   Prometheus → Alertmanager → Notification Channels (Slack, PagerDuty, Email, etc.)

groups:
  - name: singbox_dns
    interval: 30s
    rules:
      - alert: HighDNSErrorRate
        expr: |
          (sum(rate(dns_error_total[5m])) / sum(rate(dns_query_total[5m]))) > 0.05
        for: 2m
        labels:
          severity: warning
          component: dns
        annotations:
          summary: "High DNS error rate detected"
          description: "DNS error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      - alert: CriticalDNSErrorRate
        expr: |
          (sum(rate(dns_error_total[5m])) / sum(rate(dns_query_total[5m]))) > 0.20
        for: 1m
        labels:
          severity: critical
          component: dns
        annotations:
          summary: "Critical DNS error rate"
          description: "DNS error rate is {{ $value | humanizePercentage }} (threshold: 20%)"

      - alert: DNSHighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(dns_rtt_ms_bucket[5m])) by (le)) > 500
        for: 5m
        labels:
          severity: warning
          component: dns
        annotations:
          summary: "DNS P95 latency is high"
          description: "DNS P95 RTT is {{ $value | humanize }}ms (threshold: 500ms)"

      - alert: DNSCacheLowHitRate
        expr: |
          (sum(rate(dns_cache_hit_total[5m])) / (sum(rate(dns_cache_hit_total[5m])) + sum(rate(dns_cache_miss_total[5m])))) < 0.5
        for: 10m
        labels:
          severity: info
          component: dns
        annotations:
          summary: "DNS cache hit rate is low"
          description: "DNS cache hit rate is {{ $value | humanizePercentage }} (threshold: 50%)"

  - name: singbox_http
    interval: 30s
    rules:
      - alert: HighHTTPErrorRate
        expr: |
          (sum(rate(http_respond_total{code=~"5.."}[5m])) / sum(rate(http_respond_total[5m]))) > 0.05
        for: 2m
        labels:
          severity: warning
          component: http
        annotations:
          summary: "High HTTP 5xx error rate"
          description: "HTTP 5xx error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      - alert: CriticalHTTPErrorRate
        expr: |
          (sum(rate(http_respond_total{code=~"5.."}[5m])) / sum(rate(http_respond_total[5m]))) > 0.20
        for: 1m
        labels:
          severity: critical
          component: http
        annotations:
          summary: "Critical HTTP 5xx error rate"
          description: "HTTP 5xx error rate is {{ $value | humanizePercentage }} (threshold: 20%)"

      - alert: HTTPHighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 5
        for: 5m
        labels:
          severity: warning
          component: http
        annotations:
          summary: "HTTP P95 latency is high"
          description: "HTTP P95 latency is {{ $value | humanizeDuration }}s (threshold: 5s)"

      - alert: HighHTTPClientErrorRate
        expr: |
          (sum(rate(http_respond_total{code=~"4.."}[5m])) / sum(rate(http_respond_total[5m]))) > 0.30
        for: 5m
        labels:
          severity: info
          component: http
        annotations:
          summary: "High HTTP 4xx error rate"
          description: "HTTP 4xx error rate is {{ $value | humanizePercentage }} (threshold: 30%)"

  - name: singbox_outbound
    interval: 30s
    rules:
      - alert: HighOutboundFailureRate
        expr: |
          (sum(rate(outbound_connect_total{result!="success"}[5m])) / sum(rate(outbound_connect_total[5m]))) > 0.10
        for: 3m
        labels:
          severity: warning
          component: outbound
        annotations:
          summary: "High outbound connection failure rate"
          description: "Outbound failure rate is {{ $value | humanizePercentage }} (threshold: 10%)"

      - alert: CriticalOutboundFailureRate
        expr: |
          (sum(rate(outbound_connect_total{result!="success"}[5m])) / sum(rate(outbound_connect_total[5m]))) > 0.30
        for: 1m
        labels:
          severity: critical
          component: outbound
        annotations:
          summary: "Critical outbound connection failure rate"
          description: "Outbound failure rate is {{ $value | humanizePercentage }} (threshold: 30%)"

      - alert: ProxyEndpointDown
        expr: |
          proxy_up == 0
        for: 1m
        labels:
          severity: warning
          component: proxy
        annotations:
          summary: "Proxy endpoint is down"
          description: "Proxy endpoint {{ $labels.pool }}/{{ $labels.endpoint }} is unreachable"

      - alert: HighTCPConnectLatency
        expr: |
          histogram_quantile(0.95, sum(rate(tcp_connect_duration_seconds_bucket[5m])) by (le)) > 3
        for: 5m
        labels:
          severity: warning
          component: outbound
        annotations:
          summary: "TCP connect P95 latency is high"
          description: "TCP connect P95 latency is {{ $value | humanizeDuration }}s (threshold: 3s)"

  - name: singbox_udp
    interval: 30s
    rules:
      - alert: UDPNATTableNearCapacity
        expr: |
          udp_nat_size > 8000
        for: 2m
        labels:
          severity: warning
          component: udp
        annotations:
          summary: "UDP NAT table near capacity"
          description: "UDP NAT table size is {{ $value }} (threshold: 8000)"

      - alert: UDPNATTableFull
        expr: |
          udp_nat_size >= 9500
        for: 30s
        labels:
          severity: critical
          component: udp
        annotations:
          summary: "UDP NAT table at capacity"
          description: "UDP NAT table size is {{ $value }} (threshold: 9500)"

      - alert: HighUDPFailureRate
        expr: |
          sum(rate(udp_upstream_fail_total[5m])) > 10
        for: 3m
        labels:
          severity: warning
          component: udp
        annotations:
          summary: "High UDP failure rate"
          description: "UDP failures are {{ $value | humanize }}/s (threshold: 10/s)"

      - alert: HighUDPEvictionRate
        expr: |
          sum(rate(udp_nat_evicted_total{reason="capacity"}[5m])) > 5
        for: 5m
        labels:
          severity: warning
          component: udp
        annotations:
          summary: "High UDP NAT eviction rate due to capacity"
          description: "UDP NAT capacity evictions are {{ $value | humanize }}/s (threshold: 5/s)"

  - name: singbox_system
    interval: 30s
    rules:
      - alert: HighMemoryUsage
        expr: |
          process_resident_memory_bytes > 1073741824  # 1GB
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize1024 }}B (threshold: 1GB)"

      - alert: CriticalMemoryUsage
        expr: |
          process_resident_memory_bytes > 2147483648  # 2GB
        for: 2m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Critical memory usage"
          description: "Memory usage is {{ $value | humanize1024 }}B (threshold: 2GB)"

      - alert: HighCPUUsage
        expr: |
          rate(process_cpu_seconds_total[1m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: system
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"

      - alert: SingBoxDown
        expr: |
          up{job="singbox-rust"} == 0
        for: 1m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "SingBox-Rust is down"
          description: "SingBox-Rust instance is not responding to Prometheus scrapes"

      - alert: PrometheusExportFailed
        expr: |
          increase(prom_http_fail_total[5m]) > 5
        for: 2m
        labels:
          severity: warning
          component: metrics
        annotations:
          summary: "Prometheus metrics export failures detected"
          description: "Metrics export has failed {{ $value }} times in the last 5 minutes"

  - name: singbox_routing
    interval: 30s
    rules:
      - alert: HighCircuitBreakerActivation
        expr: |
          sum(rate(proxy_circuit_state_total{state="open"}[5m])) > 0.5
        for: 2m
        labels:
          severity: warning
          component: routing
        annotations:
          summary: "High circuit breaker activation rate"
          description: "Circuit breakers are opening at {{ $value | humanize }}/s"

      - alert: HighAdapterRetryRate
        expr: |
          rate(adapter_retries_total[5m]) > 1
        for: 5m
        labels:
          severity: info
          component: routing
        annotations:
          summary: "High adapter retry rate"
          description: "Adapter retries are occurring at {{ $value | humanize }}/s"

# Alertmanager Configuration Example:
#
# Create /etc/prometheus/alertmanager.yml:
#
# global:
#   resolve_timeout: 5m
#
# route:
#   group_by: ['alertname', 'component']
#   group_wait: 10s
#   group_interval: 10s
#   repeat_interval: 12h
#   receiver: 'default'
#
#   routes:
#     - match:
#         severity: critical
#       receiver: 'pagerduty'
#       continue: true
#     - match:
#         severity: warning
#       receiver: 'slack'
#
# receivers:
#   - name: 'default'
#     email_configs:
#       - to: 'ops@example.com'
#
#   - name: 'slack'
#     slack_configs:
#       - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
#         channel: '#alerts'
#         title: 'SingBox-Rust Alert'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}: {{ .Annotations.description }}{{ end }}'
#
#   - name: 'pagerduty'
#     pagerduty_configs:
#       - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'

# Alert Severity Levels:
#
# - critical: Immediate action required (page on-call)
# - warning: Needs attention (Slack notification)
# - info: Informational (logged for review)

# Testing Alerts:
#
# 1. Check alert rules: curl http://localhost:9090/api/v1/rules
# 2. Trigger test alert: Modify thresholds temporarily
# 3. View firing alerts: http://localhost:9090/alerts
# 4. Silence alerts: http://localhost:9093/#/silences (Alertmanager UI)
